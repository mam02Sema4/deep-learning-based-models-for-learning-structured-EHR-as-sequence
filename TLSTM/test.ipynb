{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T23:46:22.796011Z",
     "start_time": "2020-07-24T23:46:22.792193Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.5.0', '2.2.0')"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import tensorflow as tf\n",
    "torch.__version__, tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T20:04:39.428961Z",
     "start_time": "2020-07-24T20:04:39.425082Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=1.0>, TensorShape([]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(1, dtype=tf.float32), tf.constant(1, dtype=tf.float32).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T20:04:35.966571Z",
     "start_time": "2020-07-24T20:04:35.961892Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.), torch.Size([]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(2, dtype=torch.float32), torch.tensor(2, dtype=torch.float32).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T20:12:26.299566Z",
     "start_time": "2020-07-24T20:12:26.294616Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 10), dtype=float32, numpy=array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], dtype=float32)>,\n",
       " TensorShape([1, 10]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.ones([1, 10], dtype=tf.float32), tf.ones([1, 10], dtype=tf.float32).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T20:12:29.750002Z",
     "start_time": "2020-07-24T20:12:29.745095Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]), torch.Size([1, 10]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones((1,10), dtype=torch.float32), torch.ones((1,10), dtype=torch.float32).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T20:29:25.866561Z",
     "start_time": "2020-07-24T20:29:25.859754Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[9.951922]], dtype=float32)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.math.log(tf.constant([[0.2, 0.5, 1, 5]]) + tf.constant(1.0))\n",
    "b = tf.math.divide(tf.constant(1.0), a)\n",
    "c = tf.ones([1, 4], tf.float32)\n",
    "b.shape, tf.ones([1, 4]).shape\n",
    "tf.matmul(b, tf.ones([4, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T20:30:39.718067Z",
     "start_time": "2020-07-24T20:30:39.711342Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[9.9519]]), tensor([[9.9519]]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.log(torch.tensor([[0.2, 0.5, 1, 5]]) + torch.tensor(1, dtype=torch.float32))\n",
    "b = torch.div(torch.tensor(1, dtype=torch.float32), a)\n",
    "c = torch.ones((4,1), dtype=torch.float32)\n",
    "b @ c, torch.matmul(b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T23:45:07.657227Z",
     "start_time": "2020-07-24T23:45:07.652840Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.1823, 0.4055],\n",
       "        [0.6931, 1.7918]], requires_grad=True)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Parameter(torch.Tensor(2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T02:14:23.177675Z",
     "start_time": "2020-07-25T02:14:23.168099Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.6647],\n",
      "         [0.1582],\n",
      "         [0.0415]],\n",
      "\n",
      "        [[0.3778],\n",
      "         [0.9922],\n",
      "         [0.2763]]]) tensor([[[0.5134, 0.9242, 0.4529, 0.6371],\n",
      "         [0.4035, 0.0728, 0.4480, 0.2958],\n",
      "         [0.9435, 0.3630, 0.8144, 0.9952]],\n",
      "\n",
      "        [[0.9117, 0.9150, 0.2996, 0.6063],\n",
      "         [0.3567, 0.9934, 0.8267, 0.9316],\n",
      "         [0.4794, 0.6481, 0.3292, 0.1876]]])\n",
      "tensor([[[0.6647, 0.5134, 0.9242, 0.4529, 0.6371],\n",
      "         [0.1582, 0.4035, 0.0728, 0.4480, 0.2958],\n",
      "         [0.0415, 0.9435, 0.3630, 0.8144, 0.9952]],\n",
      "\n",
      "        [[0.3778, 0.9117, 0.9150, 0.2996, 0.6063],\n",
      "         [0.9922, 0.3567, 0.9934, 0.8267, 0.9316],\n",
      "         [0.2763, 0.4794, 0.6481, 0.3292, 0.1876]]])\n",
      "index:  0\n",
      "tensor([[0.6647, 0.5134, 0.9242, 0.4529, 0.6371],\n",
      "        [0.3778, 0.9117, 0.9150, 0.2996, 0.6063]])\n",
      "index:  1\n",
      "tensor([[0.1582, 0.4035, 0.0728, 0.4480, 0.2958],\n",
      "        [0.9922, 0.3567, 0.9934, 0.8267, 0.9316]])\n",
      "index:  2\n",
      "tensor([[0.0415, 0.9435, 0.3630, 0.8144, 0.9952],\n",
      "        [0.2763, 0.4794, 0.6481, 0.3292, 0.1876]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand((2,3,1))\n",
    "b = torch.rand((2,3,4))\n",
    "print(a, b)\n",
    "\n",
    "c = torch.cat((a, b), dim=2)\n",
    "print(c)\n",
    "\n",
    "for t in range(3):\n",
    "    print(\"index: \", t)\n",
    "    print(c[:, t, :])\n",
    "#     print(c[:, t, 0])\n",
    "#     print(c[:, t, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T14:43:57.810489Z",
     "start_time": "2020-07-25T14:43:57.805879Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3, 4), dtype=float32, numpy=\n",
       "array([[[0.3412673 , 0.6143132 , 0.30103528, 0.42347988],\n",
       "        [0.06384967, 0.01151462, 0.07089078, 0.04680625],\n",
       "        [0.03913313, 0.01505405, 0.03377762, 0.04127943]],\n",
       "\n",
       "       [[0.34445944, 0.3457046 , 0.11317814, 0.22907218],\n",
       "        [0.35396236, 0.98564976, 0.82023317, 0.9243021 ],\n",
       "        [0.13246745, 0.17909516, 0.09095483, 0.05182858]]], dtype=float32)>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.multiply(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T14:48:33.641924Z",
     "start_time": "2020-07-25T14:48:33.637759Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3413, 0.6143, 0.3010, 0.4235],\n",
       "         [0.0638, 0.0115, 0.0709, 0.0468],\n",
       "         [0.0391, 0.0151, 0.0338, 0.0413]],\n",
       "\n",
       "        [[0.3445, 0.3457, 0.1132, 0.2291],\n",
       "         [0.3540, 0.9856, 0.8202, 0.9243],\n",
       "         [0.1325, 0.1791, 0.0910, 0.0518]]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mul(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T01:41:51.693642Z",
     "start_time": "2020-07-26T01:41:51.687487Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4]) tensor([[[0.5134, 0.9242, 0.4529, 0.6371],\n",
      "         [0.4035, 0.0728, 0.4480, 0.2958],\n",
      "         [0.9435, 0.3630, 0.8144, 0.9952]],\n",
      "\n",
      "        [[0.9117, 0.9150, 0.2996, 0.6063],\n",
      "         [0.3567, 0.9934, 0.8267, 0.9316],\n",
      "         [0.4794, 0.6481, 0.3292, 0.1876]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2, 3, 4]),\n",
       " tensor([[[[0.5134, 0.9242, 0.4529, 0.6371],\n",
       "           [0.4035, 0.0728, 0.4480, 0.2958],\n",
       "           [0.9435, 0.3630, 0.8144, 0.9952]],\n",
       " \n",
       "          [[0.9117, 0.9150, 0.2996, 0.6063],\n",
       "           [0.3567, 0.9934, 0.8267, 0.9316],\n",
       "           [0.4794, 0.6481, 0.3292, 0.1876]]]]))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(b.shape, b)\n",
    "bu = torch.unsqueeze(b, 0)\n",
    "bu.shape, bu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T01:43:34.778530Z",
     "start_time": "2020-07-26T01:43:34.773394Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 2, 3, 4]),\n",
       " tensor([[[[0.5134, 0.9242, 0.4529, 0.6371],\n",
       "           [0.4035, 0.0728, 0.4480, 0.2958],\n",
       "           [0.9435, 0.3630, 0.8144, 0.9952]],\n",
       " \n",
       "          [[0.9117, 0.9150, 0.2996, 0.6063],\n",
       "           [0.3567, 0.9934, 0.8267, 0.9316],\n",
       "           [0.4794, 0.6481, 0.3292, 0.1876]]],\n",
       " \n",
       " \n",
       "         [[[0.5134, 0.9242, 0.4529, 0.6371],\n",
       "           [0.4035, 0.0728, 0.4480, 0.2958],\n",
       "           [0.9435, 0.3630, 0.8144, 0.9952]],\n",
       " \n",
       "          [[0.9117, 0.9150, 0.2996, 0.6063],\n",
       "           [0.3567, 0.9934, 0.8267, 0.9316],\n",
       "           [0.4794, 0.6481, 0.3292, 0.1876]]]]))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buc = torch.cat([bu, bu], 0)\n",
    "buc.shape, buc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T01:45:55.122582Z",
     "start_time": "2020-07-26T01:45:55.117790Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.5134, 0.9242, 0.4529, 0.6371],\n",
       "          [0.4035, 0.0728, 0.4480, 0.2958],\n",
       "          [0.9435, 0.3630, 0.8144, 0.9952]],\n",
       "\n",
       "         [[0.5134, 0.9242, 0.4529, 0.6371],\n",
       "          [0.4035, 0.0728, 0.4480, 0.2958],\n",
       "          [0.9435, 0.3630, 0.8144, 0.9952]]],\n",
       "\n",
       "\n",
       "        [[[0.9117, 0.9150, 0.2996, 0.6063],\n",
       "          [0.3567, 0.9934, 0.8267, 0.9316],\n",
       "          [0.4794, 0.6481, 0.3292, 0.1876]],\n",
       "\n",
       "         [[0.9117, 0.9150, 0.2996, 0.6063],\n",
       "          [0.3567, 0.9934, 0.8267, 0.9316],\n",
       "          [0.4794, 0.6481, 0.3292, 0.1876]]]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buc.transpose(0, 1).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import IntEnum\n",
    "class Dim(IntEnum):\n",
    "    batch = 0\n",
    "    seq = 1\n",
    "    feature = 2\n",
    "    \n",
    "class NaiveLSTM(nn.Module):\n",
    "    def __init__(self, input_sz: int, hidden_sz: int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_sz\n",
    "        self.hidden_size = hidden_sz\n",
    "        # input gate\n",
    "        self.W_ii = Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.W_hi = Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.b_i = Parameter(torch.Tensor(hidden_sz))\n",
    "        # forget gate\n",
    "        self.W_if = Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.W_hf = Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.b_f = Parameter(torch.Tensor(hidden_sz))\n",
    "        # ???\n",
    "        self.W_ig = Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.W_hg = Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.b_g = Parameter(torch.Tensor(hidden_sz))\n",
    "        # output gate\n",
    "        self.W_io = Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.W_ho = Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.b_o = Parameter(torch.Tensor(hidden_sz))\n",
    "         \n",
    "        self.init_weights()\n",
    "     \n",
    "    def init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.data.ndimension() >= 2:\n",
    "                nn.init.xavier_uniform_(p.data)\n",
    "            else:\n",
    "                nn.init.zeros_(p.data)\n",
    "         \n",
    "    def forward(self, x: torch.Tensor, init_states=None):\n",
    "        \"\"\"Assumes x is of shape (batch, sequence, feature)\"\"\"\n",
    "        bs, seq_sz, _ = x.size()\n",
    "        hidden_seq = []\n",
    "        if init_states is None:\n",
    "            h_t, c_t = torch.zeros(self.hidden_size).to(x.device), torch.zeros(self.hidden_size).to(x.device)\n",
    "        else:\n",
    "            h_t, c_t = init_states\n",
    "        for t in range(seq_sz): # iterate over the time steps\n",
    "            x_t = x[:, t, :]\n",
    "            i_t = torch.sigmoid(x_t @ self.W_ii + h_t @ self.W_hi + self.b_i)\n",
    "            f_t = torch.sigmoid(x_t @ self.W_if + h_t @ self.W_hf + self.b_f)\n",
    "            g_t = torch.tanh(x_t @ self.W_ig + h_t @ self.W_hg + self.b_g)\n",
    "            o_t = torch.sigmoid(x_t @ self.W_io + h_t @ self.W_ho + self.b_o)\n",
    "            c_t = f_t * c_t + i_t * g_t\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "            print(h_t.shape)\n",
    "            print(c_t.shape)\n",
    "            print(h_t.unsqueeze(Dim.batch).shape)\n",
    "#             hidden_seq.append(h_t.unsqueeze(Dim.batch))\n",
    "            \n",
    "            hidden_seq.append(torch.stack((h_t, c_t)))\n",
    "            \n",
    "        hidden_seq = torch.cat(hidden_seq, dim=Dim.batch)\n",
    "        print(hidden_seq.shape)\n",
    "        # reshape from shape (sequence, batch, feature) to (batch, sequence, feature)\n",
    "        hidden_seq = hidden_seq.transpose(Dim.batch, Dim.seq).contiguous()\n",
    "        return hidden_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = NaiveLSTM(4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "torch.Size([3, 4])\n",
      "torch.Size([1, 3, 4])\n",
      "torch.Size([3, 4])\n",
      "torch.Size([3, 4])\n",
      "torch.Size([1, 3, 4])\n",
      "torch.Size([4, 3, 4])\n",
      "torch.Size([3, 4])\n",
      "torch.Size([3, 4])\n",
      "torch.Size([1, 3, 4])\n",
      "torch.Size([3, 4])\n",
      "torch.Size([3, 4])\n",
      "torch.Size([1, 3, 4])\n",
      "torch.Size([4, 3, 4])\n",
      "torch.Size([3, 4])\n",
      "torch.Size([3, 4])\n",
      "torch.Size([1, 3, 4])\n",
      "torch.Size([3, 4])\n",
      "torch.Size([3, 4])\n",
      "torch.Size([1, 3, 4])\n",
      "torch.Size([4, 3, 4])\n",
      "torch.Size([3, 4])\n",
      "torch.Size([3, 4])\n",
      "torch.Size([1, 3, 4])\n",
      "torch.Size([3, 4])\n",
      "torch.Size([3, 4])\n",
      "torch.Size([1, 3, 4])\n",
      "torch.Size([4, 3, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1.3717e-01,  1.0376e-01,  1.3320e-01, -1.1520e-02],\n",
       "          [-2.9503e-01,  2.6309e-01,  2.9617e-01, -4.3014e-02],\n",
       "          [-1.9200e-01,  1.3171e-01,  2.1525e-01, -7.0139e-02],\n",
       "          [-3.3907e-01,  3.6018e-01,  4.3895e-01, -1.8758e-01]],\n",
       " \n",
       "         [[-3.4174e-03,  1.2078e-01,  1.3161e-01, -4.3694e-02],\n",
       "          [-6.1657e-03,  3.0510e-01,  2.3214e-01, -1.0799e-01],\n",
       "          [ 3.7725e-02,  1.2550e-01,  1.1936e-01, -7.3158e-02],\n",
       "          [ 5.4373e-02,  4.0061e-01,  2.0882e-01, -1.4731e-01]],\n",
       " \n",
       "         [[ 2.9614e-02,  8.8284e-02,  8.3265e-02,  1.4915e-04],\n",
       "          [ 6.0430e-02,  1.9572e-01,  1.6590e-01,  3.6722e-04],\n",
       "          [-1.8403e-01,  1.0382e-01,  9.9795e-02,  3.3597e-02],\n",
       "          [-3.2940e-01,  3.1106e-01,  2.7646e-01,  1.1721e-01]]],\n",
       "        grad_fn=<CopyBackwards>),\n",
       " torch.Size([3, 4, 4]),\n",
       " tensor([[-0.3391,  0.3602,  0.4390, -0.1876],\n",
       "         [ 0.0544,  0.4006,  0.2088, -0.1473],\n",
       "         [-0.3294,  0.3111,  0.2765,  0.1172]], grad_fn=<SliceBackward>),\n",
       " torch.Size([3, 4]))"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b.shape (2,3,4)\n",
    "lstm(b), lstm(b).shape,  lstm(b)[:, -1, :],  lstm(b)[:, -1, :].shape\n",
    "# lstm(b)[:, 0, :, :], lstm(b).shape, lstm(b)[:, 0, :, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "torch.Size([3, 4])\n",
      "torch.Size([1, 3, 4])\n",
      "torch.Size([3, 4])\n",
      "torch.Size([3, 4])\n",
      "torch.Size([1, 3, 4])\n",
      "torch.Size([4, 3, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([2, 1, 1], grad_fn=<NotImplemented>)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(lstm(b)[:, -1, :], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7683)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.BCEWithLogitsLoss()(torch.tensor([[0.1, 0.8],[0.3, 0.7]]), torch.tensor([[1,0], [0,1]],dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexgre/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.1032],\n",
       "        [0.5130]])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class masked_softmax_cross_entropy_loss(nn.Module):\n",
    "    def __init__(self, weight=None):\n",
    "        super(masked_softmax_cross_entropy_loss, self).__init__()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if not target.is_same_size(input):\n",
    "            raise ValueError(\"Target size ({}) must be the same as input size ({})\".format(target.size(), input.size()))\n",
    "\n",
    "        input = F.softmax(input)\n",
    "        loss = -torch.sum(target * torch.log(input), 1)\n",
    "        loss = torch.unsqueeze(loss, 1)\n",
    "        return loss\n",
    "    \n",
    "masked_softmax_cross_entropy_loss()(torch.tensor([[0.1, 0.8],[0.3, 0.7]]), torch.tensor([[1,0], [0,1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1.1031861, 0.5130153], dtype=float32)>"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.softmax_cross_entropy_with_logits(labels=[[1,0],[0,1]], logits=[[0.1, 0.8],[0.3, 0.7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch import nn as N\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TLSTMConfig:\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim, fc_dim, dropoutput_rate):\n",
    "        self.dropout_prob = dropoutput_rate\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.fc_dim = fc_dim\n",
    "\n",
    "\n",
    "class SoftmaxCrossEntropyLoss(N.Module):\n",
    "    \"\"\"\n",
    "    equivalent implementation of tf.nn.softmax_cross_entropy_with_logits\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weight=None):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        if not targets.is_same_size(inputs):\n",
    "            raise ValueError(\"Target size ({}) must be the same as input size ({})\".format(targets.size(),\n",
    "                                                                                           inputs.size()))\n",
    "\n",
    "        inputs = F.softmax(inputs)\n",
    "        loss = -torch.sum(targets * torch.log(inputs), 1)\n",
    "        loss = torch.unsqueeze(loss, 1)\n",
    "        return torch.mean(loss)\n",
    "\n",
    "\n",
    "class TLSTMCell(N.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.Wi = Parameter(torch.Tensor(input_dim, hidden_dim))\n",
    "        self.Ui = Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "        self.bi = Parameter(torch.Tensor(hidden_dim))\n",
    "\n",
    "        self.Wf = Parameter(torch.Tensor(input_dim, hidden_dim))\n",
    "        self.Uf = Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "        self.bf = Parameter(torch.Tensor(hidden_dim))\n",
    "\n",
    "        self.Wog = Parameter(torch.Tensor(input_dim, hidden_dim))\n",
    "        self.Uog = Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "        self.bog = Parameter(torch.Tensor(hidden_dim))\n",
    "\n",
    "        self.Wc = Parameter(torch.Tensor(input_dim, hidden_dim))\n",
    "        self.Uc = Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "        self.bc = Parameter(torch.Tensor(hidden_dim))\n",
    "\n",
    "        self.W_decomp = Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "        self.b_decomp = Parameter(torch.Tensor(hidden_dim))\n",
    "\n",
    "    def init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.data.ndimension() >= 2:\n",
    "                N.init.xavier_uniform_(p.data)\n",
    "            else:\n",
    "                N.init.zeros_(p.data)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, time: torch.Tensor, prev_hidden_state=None):\n",
    "        # x has three dim: batch size(pats), seq_size(encounters), v_dim(features)\n",
    "        bz, seq_sz, v_dim = x.size()\n",
    "        tbz, tseq_sz, tv_dim = time.size()\n",
    "        assert bz == tbz and seq_sz == tseq_sz, \\\n",
    "            \"feature and time seq have different batch size {}-{} or seq len {}-{}\".format(bz, tbz, seq_sz, tseq_sz)\n",
    "\n",
    "        # init hidden if no previous hidden\n",
    "        if prev_hidden_state is None:\n",
    "            h_t, c_t = torch.zeros(self.hidden_dim).to(x.device), torch.zeros(self.hidden_dim).to(x.device)\n",
    "        else:\n",
    "            h_t, c_t = prev_hidden_state\n",
    "\n",
    "        # recurrent loop\n",
    "        hidden_seq = []\n",
    "        for i, t in enumerate(range(seq_sz)):\n",
    "            x_t = x[:, t, :]  # (batch, input)\n",
    "            print(x_t.shape)\n",
    "            # process time difference\n",
    "            t_t = time[:, t, :]\n",
    "            print(t_t.shape)\n",
    "            T = self.map_elapse_time(t_t)\n",
    "            print(T.shape)\n",
    "            C_ST = torch.tanh(torch.matmul(c_t, self.W_decomp) + self.b_decomp)\n",
    "            C_ST_dis = torch.mul(T, C_ST)\n",
    "            c_t = c_t - C_ST + C_ST_dis\n",
    "            print(c_t.shape)\n",
    "            # input gate\n",
    "            i_t = torch.sigmoid(torch.matmul(x_t, self.Wi) + torch.matmul(h_t, self.Ui) + self.bi)\n",
    "            print(i_t.shape)\n",
    "            # forget gate\n",
    "            f_t = torch.sigmoid(torch.matmul(x_t, self.Wf) + torch.matmul(h_t, self.Uf) + self.bf)\n",
    "            print(f_t.shape)\n",
    "            # output gate\n",
    "            o_t = torch.sigmoid(torch.matmul(x_t, self.Wog) + torch.matmul(h_t, self.Uog) + self.bog)\n",
    "            print(o_t.shape)\n",
    "            # cadidate MemCell\n",
    "            C = torch.tanh(torch.matmul(x_t, self.Wc) + torch.matmul(h_t, self.Uc) + self.bc)\n",
    "            print(C.shape)\n",
    "            # current MemCell\n",
    "            C_t = f_t * c_t + i_t * C\n",
    "            # current hidden state\n",
    "            c_h_t = o_t * torch.tanh(C_t)\n",
    "            # hidden_seq.append(torch.stack((c_h_t, C_t)))\n",
    "            hidden_seq.append(c_h_t.unsqueeze(0))  # create extra dim for later concat (seq, batch, input)\n",
    "\n",
    "        hidden_seq = torch.cat(hidden_seq, dim=0)  # concat to get the seq dim back (seq, batch, input)\n",
    "        # hidden_seq = hidden_seq.transpose(0, 1).contiguous()  # (seq, batch, input) => (batch, seq, input)\n",
    "        return hidden_seq, (h_t, c_t)\n",
    "\n",
    "    def map_elapse_time(self, t):\n",
    "        c1 = torch.tensor(1, dtype=torch.float32)\n",
    "        c2 = torch.tensor(np.e, dtype=torch.float32)\n",
    "\n",
    "        T = torch.div(c1, torch.log(t + c2))\n",
    "        grid = torch.ones((1, self.hidden_dim), dtype=torch.float32)\n",
    "        return torch.matmul(T, grid)\n",
    "\n",
    "\n",
    "class TLSTM(N.Module):\n",
    "\n",
    "    def __init__(self, config=None):\n",
    "        super().__init__()\n",
    "        # self.fc_dim = fc_dim\n",
    "        # self.output_dim = output_dim\n",
    "        self.tlstm = TLSTMCell(config.input_dim, config.hidden_dim)\n",
    "        self.dropout_prob = config.dropout_prob\n",
    "        self.fc_layer = N.Linear(config.hidden_dim, config.fc_dim)\n",
    "        self.output_layer = N.Linear(config.fc_dim, config.output_dim)\n",
    "\n",
    "        # we can use pytorch default implementation of binary loss function - BCEWithLogitsLoss\n",
    "        # but we found the output did not exactly match the tf.nn.softmax_cross_entropy_with_logits with mean reduce\n",
    "        # To make sure, results repeatable, we will use SoftmaxCrossEntropyLoss for now\n",
    "        # self.loss_fct = N.BCEWithLogitsLoss()\n",
    "        self.loss_fct = SoftmaxCrossEntropyLoss()\n",
    "\n",
    "    def forward(self, feature, time, labels):\n",
    "        # get raw logits\n",
    "        seq, (h_t, c_t) = self.tlstm(feature, time)\n",
    "        seq = seq[-1, :, :]  # (seq, batch, input); get the last seq for classification\n",
    "        seq = F.relu(self.fc_layer(seq))\n",
    "        seq = F.dropout(seq, p=self.dropout_prob)\n",
    "        logits = self.output_layer(seq)\n",
    "\n",
    "        # measure loss\n",
    "        loss = self.loss_fct(logits, labels)\n",
    "        return loss, logits, torch.argmax(logits, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3, 4])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexgre/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/ipykernel_launcher.py:30: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.6366, grad_fn=<MeanBackward0>),\n",
       " tensor([[-0.3495,  0.3101],\n",
       "         [-0.3495,  0.3101],\n",
       "         [-0.3495,  0.3101]], grad_fn=<AddmmBackward>),\n",
       " tensor([1, 1, 1], grad_fn=<NotImplemented>))"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = TLSTMConfig(4, 2, 10, 5, 0.1)\n",
    "t = TLSTM(config)\n",
    "label = torch.tensor([[0,1], [1,0], [0,1]])\n",
    "x = torch.rand(3,2,1)\n",
    "print(b)\n",
    "t(b, , label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
