{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T23:46:22.796011Z",
     "start_time": "2020-07-24T23:46:22.792193Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.5.0', '2.2.0')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import tensorflow as tf\n",
    "torch.__version__, tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T20:04:39.428961Z",
     "start_time": "2020-07-24T20:04:39.425082Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=1.0>, TensorShape([]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(1, dtype=tf.float32), tf.constant(1, dtype=tf.float32).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T20:04:35.966571Z",
     "start_time": "2020-07-24T20:04:35.961892Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.), torch.Size([]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(2, dtype=torch.float32), torch.tensor(2, dtype=torch.float32).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T20:12:26.299566Z",
     "start_time": "2020-07-24T20:12:26.294616Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 10), dtype=float32, numpy=array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], dtype=float32)>,\n",
       " TensorShape([1, 10]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.ones([1, 10], dtype=tf.float32), tf.ones([1, 10], dtype=tf.float32).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T20:12:29.750002Z",
     "start_time": "2020-07-24T20:12:29.745095Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]), torch.Size([1, 10]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones((1,10), dtype=torch.float32), torch.ones((1,10), dtype=torch.float32).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T20:29:25.866561Z",
     "start_time": "2020-07-24T20:29:25.859754Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[9.951922]], dtype=float32)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.math.log(tf.constant([[0.2, 0.5, 1, 5]]) + tf.constant(1.0))\n",
    "b = tf.math.divide(tf.constant(1.0), a)\n",
    "c = tf.ones([1, 4], tf.float32)\n",
    "b.shape, tf.ones([1, 4]).shape\n",
    "tf.matmul(b, tf.ones([4, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T20:30:39.718067Z",
     "start_time": "2020-07-24T20:30:39.711342Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[9.9519]]), tensor([[9.9519]]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.log(torch.tensor([[0.2, 0.5, 1, 5]]) + torch.tensor(1, dtype=torch.float32))\n",
    "b = torch.div(torch.tensor(1, dtype=torch.float32), a)\n",
    "c = torch.ones((4,1), dtype=torch.float32)\n",
    "b @ c, torch.matmul(b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T23:45:07.657227Z",
     "start_time": "2020-07-24T23:45:07.652840Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.1823, 0.4055],\n",
       "        [0.6931, 1.7918]], requires_grad=True)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Parameter(torch.Tensor(2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T02:14:23.177675Z",
     "start_time": "2020-07-25T02:14:23.168099Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.9572],\n",
      "         [0.4080],\n",
      "         [0.6568]],\n",
      "\n",
      "        [[0.3190],\n",
      "         [0.7185],\n",
      "         [0.3163]]]) tensor([[[0.8835, 0.2457, 0.1546, 0.7032],\n",
      "         [0.1824, 0.5793, 0.6159, 0.9758],\n",
      "         [0.2774, 0.1703, 0.1724, 0.2960]],\n",
      "\n",
      "        [[0.3209, 0.1021, 0.8421, 0.5318],\n",
      "         [0.5956, 0.3149, 0.6690, 0.6734],\n",
      "         [0.1413, 0.3545, 0.4481, 0.5095]]])\n",
      "tensor([[[0.9572, 0.8835, 0.2457, 0.1546, 0.7032],\n",
      "         [0.4080, 0.1824, 0.5793, 0.6159, 0.9758],\n",
      "         [0.6568, 0.2774, 0.1703, 0.1724, 0.2960]],\n",
      "\n",
      "        [[0.3190, 0.3209, 0.1021, 0.8421, 0.5318],\n",
      "         [0.7185, 0.5956, 0.3149, 0.6690, 0.6734],\n",
      "         [0.3163, 0.1413, 0.3545, 0.4481, 0.5095]]])\n",
      "index:  0\n",
      "tensor([[0.9572, 0.8835, 0.2457, 0.1546, 0.7032],\n",
      "        [0.3190, 0.3209, 0.1021, 0.8421, 0.5318]])\n",
      "index:  1\n",
      "tensor([[0.4080, 0.1824, 0.5793, 0.6159, 0.9758],\n",
      "        [0.7185, 0.5956, 0.3149, 0.6690, 0.6734]])\n",
      "index:  2\n",
      "tensor([[0.6568, 0.2774, 0.1703, 0.1724, 0.2960],\n",
      "        [0.3163, 0.1413, 0.3545, 0.4481, 0.5095]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand((2,3,1))\n",
    "b = torch.rand((2,3,4))\n",
    "print(a, b)\n",
    "\n",
    "c = torch.cat((a, b), dim=2)\n",
    "print(c)\n",
    "\n",
    "for t in range(3):\n",
    "    print(\"index: \", t)\n",
    "    print(c[:, t, :])\n",
    "#     print(c[:, t, 0])\n",
    "#     print(c[:, t, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T14:43:57.810489Z",
     "start_time": "2020-07-25T14:43:57.805879Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3, 4), dtype=float32, numpy=\n",
       "array([[[0.3412673 , 0.6143132 , 0.30103528, 0.42347988],\n",
       "        [0.06384967, 0.01151462, 0.07089078, 0.04680625],\n",
       "        [0.03913313, 0.01505405, 0.03377762, 0.04127943]],\n",
       "\n",
       "       [[0.34445944, 0.3457046 , 0.11317814, 0.22907218],\n",
       "        [0.35396236, 0.98564976, 0.82023317, 0.9243021 ],\n",
       "        [0.13246745, 0.17909516, 0.09095483, 0.05182858]]], dtype=float32)>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.multiply(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T14:48:33.641924Z",
     "start_time": "2020-07-25T14:48:33.637759Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3413, 0.6143, 0.3010, 0.4235],\n",
       "         [0.0638, 0.0115, 0.0709, 0.0468],\n",
       "         [0.0391, 0.0151, 0.0338, 0.0413]],\n",
       "\n",
       "        [[0.3445, 0.3457, 0.1132, 0.2291],\n",
       "         [0.3540, 0.9856, 0.8202, 0.9243],\n",
       "         [0.1325, 0.1791, 0.0910, 0.0518]]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mul(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T01:41:51.693642Z",
     "start_time": "2020-07-26T01:41:51.687487Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4]) tensor([[[0.5134, 0.9242, 0.4529, 0.6371],\n",
      "         [0.4035, 0.0728, 0.4480, 0.2958],\n",
      "         [0.9435, 0.3630, 0.8144, 0.9952]],\n",
      "\n",
      "        [[0.9117, 0.9150, 0.2996, 0.6063],\n",
      "         [0.3567, 0.9934, 0.8267, 0.9316],\n",
      "         [0.4794, 0.6481, 0.3292, 0.1876]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2, 3, 4]),\n",
       " tensor([[[[0.5134, 0.9242, 0.4529, 0.6371],\n",
       "           [0.4035, 0.0728, 0.4480, 0.2958],\n",
       "           [0.9435, 0.3630, 0.8144, 0.9952]],\n",
       " \n",
       "          [[0.9117, 0.9150, 0.2996, 0.6063],\n",
       "           [0.3567, 0.9934, 0.8267, 0.9316],\n",
       "           [0.4794, 0.6481, 0.3292, 0.1876]]]]))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(b.shape, b)\n",
    "bu = torch.unsqueeze(b, 0)\n",
    "bu.shape, bu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T01:43:34.778530Z",
     "start_time": "2020-07-26T01:43:34.773394Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 2, 3, 4]),\n",
       " tensor([[[[0.5134, 0.9242, 0.4529, 0.6371],\n",
       "           [0.4035, 0.0728, 0.4480, 0.2958],\n",
       "           [0.9435, 0.3630, 0.8144, 0.9952]],\n",
       " \n",
       "          [[0.9117, 0.9150, 0.2996, 0.6063],\n",
       "           [0.3567, 0.9934, 0.8267, 0.9316],\n",
       "           [0.4794, 0.6481, 0.3292, 0.1876]]],\n",
       " \n",
       " \n",
       "         [[[0.5134, 0.9242, 0.4529, 0.6371],\n",
       "           [0.4035, 0.0728, 0.4480, 0.2958],\n",
       "           [0.9435, 0.3630, 0.8144, 0.9952]],\n",
       " \n",
       "          [[0.9117, 0.9150, 0.2996, 0.6063],\n",
       "           [0.3567, 0.9934, 0.8267, 0.9316],\n",
       "           [0.4794, 0.6481, 0.3292, 0.1876]]]]))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buc = torch.cat([bu, bu], 0)\n",
    "buc.shape, buc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T01:45:55.122582Z",
     "start_time": "2020-07-26T01:45:55.117790Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.5134, 0.9242, 0.4529, 0.6371],\n",
       "          [0.4035, 0.0728, 0.4480, 0.2958],\n",
       "          [0.9435, 0.3630, 0.8144, 0.9952]],\n",
       "\n",
       "         [[0.5134, 0.9242, 0.4529, 0.6371],\n",
       "          [0.4035, 0.0728, 0.4480, 0.2958],\n",
       "          [0.9435, 0.3630, 0.8144, 0.9952]]],\n",
       "\n",
       "\n",
       "        [[[0.9117, 0.9150, 0.2996, 0.6063],\n",
       "          [0.3567, 0.9934, 0.8267, 0.9316],\n",
       "          [0.4794, 0.6481, 0.3292, 0.1876]],\n",
       "\n",
       "         [[0.9117, 0.9150, 0.2996, 0.6063],\n",
       "          [0.3567, 0.9934, 0.8267, 0.9316],\n",
       "          [0.4794, 0.6481, 0.3292, 0.1876]]]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buc.transpose(0, 1).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.rand(3,2,4)\n",
    "x =  torch.tensor([[[1],[1]], [[1],[1]],[[1],[1]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import IntEnum\n",
    "class Dim(IntEnum):\n",
    "    batch = 0\n",
    "    seq = 1\n",
    "    feature = 2\n",
    "    \n",
    "class NaiveLSTM(nn.Module):\n",
    "    def __init__(self, input_sz: int, hidden_sz: int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_sz\n",
    "        self.hidden_size = hidden_sz\n",
    "        # input gate\n",
    "        self.W_ii = Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.W_hi = Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.b_i = Parameter(torch.Tensor(hidden_sz))\n",
    "        # forget gate\n",
    "        self.W_if = Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.W_hf = Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.b_f = Parameter(torch.Tensor(hidden_sz))\n",
    "        # ???\n",
    "        self.W_ig = Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.W_hg = Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.b_g = Parameter(torch.Tensor(hidden_sz))\n",
    "        # output gate\n",
    "        self.W_io = Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.W_ho = Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.b_o = Parameter(torch.Tensor(hidden_sz))\n",
    "         \n",
    "        self.init_weights()\n",
    "     \n",
    "    def init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.data.ndimension() >= 2:\n",
    "                nn.init.xavier_uniform_(p.data)\n",
    "            else:\n",
    "                nn.init.zeros_(p.data)\n",
    "         \n",
    "    def forward(self, x: torch.Tensor, init_states=None):\n",
    "        \"\"\"Assumes x is of shape (batch, sequence, feature)\"\"\"\n",
    "        bs, seq_sz, _ = x.size()\n",
    "        hidden_seq = []\n",
    "        if init_states is None:\n",
    "            h_t, c_t = torch.zeros(self.hidden_size).to(x.device), torch.zeros(self.hidden_size).to(x.device)\n",
    "        else:\n",
    "            h_t, c_t = init_states\n",
    "        for t in range(seq_sz): # iterate over the time steps\n",
    "            x_t = x[:, t, :]\n",
    "            i_t = torch.sigmoid(x_t @ self.W_ii + h_t @ self.W_hi + self.b_i)\n",
    "            f_t = torch.sigmoid(x_t @ self.W_if + h_t @ self.W_hf + self.b_f)\n",
    "            g_t = torch.tanh(x_t @ self.W_ig + h_t @ self.W_hg + self.b_g)\n",
    "            o_t = torch.sigmoid(x_t @ self.W_io + h_t @ self.W_ho + self.b_o)\n",
    "            c_t = f_t * c_t + i_t * g_t\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "            hidden_seq.append(h_t)\n",
    "            \n",
    "        hidden_seq = torch.cat(hidden_seq, dim=Dim.batch)\n",
    "        print(hidden_seq.shape)\n",
    "        # reshape from shape (sequence, batch, feature) to (batch, sequence, feature)\n",
    "        hidden_seq = hidden_seq.transpose(Dim.batch, Dim.seq).contiguous()\n",
    "        return hidden_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = NaiveLSTM(4, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1851,  0.1192,  0.0261, -0.0510, -0.1609, -0.0670, -0.0171,\n",
       "          -0.0598,  0.0649, -0.0070],\n",
       "         [ 0.3029,  0.2360,  0.0698, -0.0916, -0.3045, -0.1521, -0.0428,\n",
       "          -0.1293,  0.1110, -0.0136],\n",
       "         [ 0.1991,  0.1571,  0.0206, -0.0930, -0.2360, -0.1860,  0.0054,\n",
       "          -0.0103,  0.0414, -0.1350],\n",
       "         [ 0.3395,  0.2884,  0.0455, -0.1556, -0.4515, -0.3714,  0.0120,\n",
       "          -0.0248,  0.0776, -0.2550]],\n",
       "\n",
       "        [[ 0.1743,  0.0923,  0.0276, -0.0449, -0.1336, -0.0357, -0.0316,\n",
       "          -0.0749,  0.0770,  0.0289],\n",
       "         [ 0.2952,  0.1912,  0.0728, -0.0836, -0.2622, -0.0837, -0.0761,\n",
       "          -0.1510,  0.1335,  0.0560],\n",
       "         [ 0.1785,  0.1698,  0.0212, -0.0797, -0.1700, -0.0985, -0.0583,\n",
       "          -0.1151,  0.1218, -0.0065],\n",
       "         [ 0.2879,  0.3196,  0.0695, -0.1462, -0.3636, -0.1964, -0.1417,\n",
       "          -0.2239,  0.2124, -0.0147]],\n",
       "\n",
       "        [[ 0.1154,  0.1734,  0.0258, -0.0633, -0.1117, -0.0658, -0.0529,\n",
       "          -0.1395,  0.1037, -0.0115],\n",
       "         [ 0.1780,  0.3075,  0.1025, -0.1123, -0.2318, -0.1176, -0.1415,\n",
       "          -0.2889,  0.1758, -0.0320],\n",
       "         [ 0.0590,  0.2323,  0.0155, -0.0792, -0.1430, -0.1657, -0.0392,\n",
       "          -0.0856,  0.0955, -0.1224],\n",
       "         [ 0.0925,  0.4062,  0.0462, -0.1386, -0.2841, -0.2854, -0.0918,\n",
       "          -0.1938,  0.1782, -0.3215]]], grad_fn=<CopyBackwards>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = torch.tensor([[0,1], [1,0], [0,1]])\n",
    "lstm(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "torch.Size([3, 4])\n",
      "torch.Size([1, 3, 4])\n",
      "torch.Size([3, 4])\n",
      "torch.Size([3, 4])\n",
      "torch.Size([1, 3, 4])\n",
      "torch.Size([4, 3, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([2, 1, 1], grad_fn=<NotImplemented>)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(lstm(b)[:, -1, :], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7683)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.BCEWithLogitsLoss()(torch.tensor([[0.1, 0.8],[0.3, 0.7]]), torch.tensor([[1,0], [0,1]],dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexgre/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.1032],\n",
       "        [0.5130]])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class masked_softmax_cross_entropy_loss(nn.Module):\n",
    "    def __init__(self, weight=None):\n",
    "        super(masked_softmax_cross_entropy_loss, self).__init__()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if not target.is_same_size(input):\n",
    "            raise ValueError(\"Target size ({}) must be the same as input size ({})\".format(target.size(), input.size()))\n",
    "\n",
    "        input = F.softmax(input)\n",
    "        loss = -torch.sum(target * torch.log(input), 1)\n",
    "        loss = torch.unsqueeze(loss, 1)\n",
    "        return loss\n",
    "    \n",
    "masked_softmax_cross_entropy_loss()(torch.tensor([[0.1, 0.8],[0.3, 0.7]]), torch.tensor([[1,0], [0,1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1.1031861, 0.5130153], dtype=float32)>"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.softmax_cross_entropy_with_logits(labels=[[1,0],[0,1]], logits=[[0.1, 0.8],[0.3, 0.7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch import nn as N\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TLSTMConfig:\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim, fc_dim, dropoutput_rate):\n",
    "        self.dropout_prob = dropoutput_rate\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.fc_dim = fc_dim\n",
    "\n",
    "\n",
    "class SoftmaxCrossEntropyLoss(N.Module):\n",
    "    \"\"\"\n",
    "    equivalent implementation of tf.nn.softmax_cross_entropy_with_logits\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weight=None):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        if not targets.is_same_size(inputs):\n",
    "            raise ValueError(\"Target size ({}) must be the same as input size ({})\".format(targets.size(),\n",
    "                                                                                           inputs.size()))\n",
    "\n",
    "        inputs = F.softmax(inputs, dim=1)\n",
    "        loss = -torch.sum(targets * torch.log(inputs), 1)\n",
    "        loss = torch.unsqueeze(loss, 1)\n",
    "        return torch.mean(loss)\n",
    "\n",
    "\n",
    "class TLSTMCell(N.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.Wi = Parameter(torch.Tensor(input_dim, hidden_dim))\n",
    "        self.Ui = Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "        self.bi = Parameter(torch.Tensor(hidden_dim))\n",
    "\n",
    "        self.Wf = Parameter(torch.Tensor(input_dim, hidden_dim))\n",
    "        self.Uf = Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "        self.bf = Parameter(torch.Tensor(hidden_dim))\n",
    "\n",
    "        self.Wog = Parameter(torch.Tensor(input_dim, hidden_dim))\n",
    "        self.Uog = Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "        self.bog = Parameter(torch.Tensor(hidden_dim))\n",
    "\n",
    "        self.Wc = Parameter(torch.Tensor(input_dim, hidden_dim))\n",
    "        self.Uc = Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "        self.bc = Parameter(torch.Tensor(hidden_dim))\n",
    "\n",
    "        self.W_decomp = Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "        self.b_decomp = Parameter(torch.Tensor(hidden_dim))\n",
    "\n",
    "    def init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.data.ndimension() >= 2:\n",
    "                N.init.xavier_uniform_(p.data)\n",
    "            else:\n",
    "                N.init.zeros_(p.data)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, time: torch.Tensor, prev_hidden_state=None):\n",
    "        # x has three dim: batch size(pats), seq_size(encounters), v_dim(features)\n",
    "        bz, seq_sz, v_dim = x.size()\n",
    "        tbz, tseq_sz, tv_dim = time.size()\n",
    "        assert bz == tbz and seq_sz == tseq_sz, \\\n",
    "            \"feature and time seq have different batch size {}-{} or seq len {}-{}\".format(bz, tbz, seq_sz, tseq_sz)\n",
    "\n",
    "        # init hidden if no previous hidden\n",
    "        if prev_hidden_state is None:\n",
    "            h_t, c_t = torch.zeros(self.hidden_dim).to(x.device), torch.zeros(self.hidden_dim).to(x.device)\n",
    "        else:\n",
    "            h_t, c_t = prev_hidden_state\n",
    "\n",
    "        # recurrent loop\n",
    "        hidden_seq = []\n",
    "        for i, t in enumerate(range(seq_sz)):\n",
    "            x_t = x[:, t, :]  # (batch, input)\n",
    "            # process time difference\n",
    "            t_t = time[:, t, :]\n",
    "            T = self.map_elapse_time(t_t)\n",
    "            C_ST = torch.tanh(torch.matmul(c_t, self.W_decomp) + self.b_decomp)\n",
    "            C_ST_dis = torch.mul(T, C_ST)\n",
    "            c_t = c_t - C_ST + C_ST_dis\n",
    "            # input gate\n",
    "            i_t = torch.sigmoid(torch.matmul(x_t, self.Wi) + torch.matmul(h_t, self.Ui) + self.bi)\n",
    "            # forget gate\n",
    "            f_t = torch.sigmoid(torch.matmul(x_t, self.Wf) + torch.matmul(h_t, self.Uf) + self.bf)\n",
    "            # output gate\n",
    "            o_t = torch.sigmoid(torch.matmul(x_t, self.Wog) + torch.matmul(h_t, self.Uog) + self.bog)\n",
    "            # cadidate MemCell\n",
    "            C = torch.tanh(torch.matmul(x_t, self.Wc) + torch.matmul(h_t, self.Uc) + self.bc)\n",
    "            # current MemCell\n",
    "            c_t = f_t * c_t + i_t * C\n",
    "            # current hidden state\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "            # hidden_seq.append(torch.stack((c_h_t, C_t)))\n",
    "            hidden_seq.append(h_t.unsqueeze(0))  # create extra dim for later concat (seq, batch, input)\n",
    "\n",
    "        hidden_seq = torch.cat(hidden_seq, dim=0)  # concat to get the seq dim back (seq, batch, input)\n",
    "        # hidden_seq = hidden_seq.transpose(0, 1).contiguous()  # (seq, batch, input) => (batch, seq, input)\n",
    "        return hidden_seq, (h_t, c_t)\n",
    "    \n",
    "    def map_elapse_time(self, t):\n",
    "        c1 = torch.tensor(1, dtype=torch.float32)\n",
    "        c2 = torch.tensor(np.e, dtype=torch.float32)\n",
    "\n",
    "        T = torch.div(c1, torch.log(t + c2))\n",
    "        grid = torch.ones((1, self.hidden_dim), dtype=torch.float32)\n",
    "        return torch.matmul(T, grid)\n",
    "\n",
    "class TLSTM(N.Module):\n",
    "\n",
    "    def __init__(self, config=None):\n",
    "        super().__init__()\n",
    "        # self.fc_dim = fc_dim\n",
    "        # self.output_dim = output_dim\n",
    "        self.tlstm = TLSTMCell(config.input_dim, config.hidden_dim)\n",
    "        self.dropout_prob = config.dropout_prob\n",
    "        self.fc_layer = N.Linear(config.hidden_dim, config.fc_dim)\n",
    "        self.output_layer = N.Linear(config.fc_dim, config.output_dim)\n",
    "\n",
    "        # we can use pytorch default implementation of binary loss function - BCEWithLogitsLoss\n",
    "        # but we found the output did not exactly match the tf.nn.softmax_cross_entropy_with_logits with mean reduce\n",
    "        # To make sure, results repeatable, we will use SoftmaxCrossEntropyLoss for now\n",
    "        # self.loss_fct = N.BCEWithLogitsLoss()\n",
    "        self.loss_fct = SoftmaxCrossEntropyLoss()\n",
    "\n",
    "    def forward(self, feature, time, labels):\n",
    "        # get raw logits\n",
    "        seq, (h_t, c_t) = self.tlstm(feature, time)\n",
    "        print(seq)\n",
    "        seq = seq[-1, :, :]  # (seq, batch, input); get the last seq for classification\n",
    "        seq = F.relu(self.fc_layer(seq))\n",
    "        seq = F.dropout(seq, p=self.dropout_prob)\n",
    "        logits = self.output_layer(seq)\n",
    "\n",
    "        # measure loss\n",
    "        loss = self.loss_fct(logits, labels)\n",
    "        return loss, logits, torch.argmax(logits, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3449, 0.7516, 0.2171, 0.6260],\n",
      "         [0.2139, 0.1072, 0.4425, 0.8249]],\n",
      "\n",
      "        [[0.2275, 0.8012, 0.0875, 0.4292],\n",
      "         [0.2506, 0.6859, 0.4641, 0.1920]],\n",
      "\n",
      "        [[0.4290, 0.7395, 0.8287, 0.0920],\n",
      "         [0.3620, 0.0878, 0.8829, 0.1919]]]) tensor([[[1],\n",
      "         [1]],\n",
      "\n",
      "        [[1],\n",
      "         [1]],\n",
      "\n",
      "        [[1],\n",
      "         [1]]])\n",
      "tensor([[[ 9.8402e-15,  3.6453e-20,  2.8377e-14,  3.6453e-20,  1.7561e-14,\n",
      "           4.4449e-42,  5.1630e-14,  1.3066e-41,  2.3253e-14,  5.8855e-42],\n",
      "         [ 3.9684e-15,  3.3271e-20,  1.9458e-14,  3.3271e-20,  8.6429e-15,\n",
      "           2.1874e-42,  4.4964e-14,  1.1379e-41,  2.5505e-14,  6.4544e-42],\n",
      "         [ 3.7568e-14,  3.8733e-20,  4.1721e-15,  3.8733e-20, -6.6434e-15,\n",
      "          -1.6816e-42,  2.6878e-14,  6.8019e-42,  2.2706e-14,  5.7453e-42]],\n",
      "\n",
      "        [[ 2.4980e-14,  5.1130e-20,  5.1581e-14,  5.1130e-20,  3.5358e-14,\n",
      "           8.9473e-42,  5.7250e-14,  1.4487e-41,  5.6684e-15,  1.4349e-42],\n",
      "         [ 2.3021e-14,  5.0533e-20,  1.8431e-14,  5.0533e-20,  2.2079e-15,\n",
      "           5.5912e-43,  5.1461e-14,  1.3022e-41,  3.3030e-14,  8.3587e-42],\n",
      "         [ 5.8807e-14,  5.6282e-20,  1.0784e-14,  5.6282e-20, -5.4391e-15,\n",
      "          -1.3761e-42,  1.5304e-14,  3.8732e-42,  4.5196e-15,  1.1449e-42]]],\n",
      "       grad_fn=<CatBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.7772, grad_fn=<MeanBackward0>),\n",
       " tensor([[ 0.1713, -0.2192],\n",
       "         [ 0.1713, -0.2192],\n",
       "         [ 0.1713, -0.2192]], grad_fn=<AddmmBackward>),\n",
       " tensor([0, 0, 0], grad_fn=<NotImplemented>))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = TLSTMConfig(4, 2, 10, 5, 0.1)\n",
    "t = TLSTM(config)\n",
    "label = torch.tensor([[0,1], [1,0], [0,1]])\n",
    "print(b, x)\n",
    "t(b, x, label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
